{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bcabb1a",
   "metadata": {
    "id": "wBQg-JWsn62k"
   },
   "source": [
    "# **EEG Motor Imagery Classification Using CNN, Transformer, and MLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94474a",
   "metadata": {
    "id": "79u6myDWn7zd"
   },
   "source": [
    "## **Important Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5d339",
   "metadata": {
    "id": "-zdyP4ntn77B"
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "# from mne.io import concatenate_raws\n",
    "\n",
    "import os\n",
    "# import re\n",
    "# import io\n",
    "import cv2\n",
    "# import random\n",
    "# import string\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# from google.colab.patches import cv2_imshow\n",
    "from cv2_plt_imshow import cv2_plt_imshow as cv2_imshow\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "from torchmetrics.classification import Accuracy\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.facecolor'] = 'lightgray'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275bfc07",
   "metadata": {
    "id": "nc-Lunzdn8CT"
   },
   "source": [
    "## **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74444494",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "doBae7vpn8IY",
    "outputId": "e16f9f35-8caf-4845-b71a-603782058a61"
   },
   "outputs": [],
   "source": [
    "global_path = ''\n",
    "dir_path_to_save_data = 'processed_data_original'\n",
    "seed_value = 42\n",
    "\n",
    "\n",
    "EEG_CHANNEL = 64\n",
    "# CLASSES = ['left_fist', 'right_fist', 'baseline_open', 'both_feet', 'both_fist']\n",
    "CLASSES = ['left_fist', 'right_fist', 'baseline_open', 'both_feet']\n",
    "CLASSES_ID = {'left_fist': 0, 'right_fist': 1, 'baseline_open': 2, 'both_feet': 3}\n",
    "no_of_classes = len(CLASSES)\n",
    "\n",
    "all_files = {}\n",
    "for dir in CLASSES:\n",
    "    all_files[dir] = [file for file in os.listdir(os.path.join(dir_path_to_save_data, dir)) if file.endswith('.fif')]\n",
    "    print(f'{dir} has {len(all_files[dir])} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fcdb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_one_subject_file(file_path, id): # return data in milli volts (input data is in micro volts)\n",
    "    epochs_eeg = mne.read_epochs(os.path.join(dir_path_to_save_data, file_path))\n",
    "    # print(np.shape(temp))\n",
    "    eeg_data = epochs_eeg.get_data(copy=False) * 1e-3\n",
    "    eeg_data = np.array(eeg_data, dtype=np.float32) \n",
    "    \n",
    "    # print(np.shape(eeg_data))\n",
    "\n",
    "    epoch = np.shape(eeg_data)[0]\n",
    "    one_action_labels = np.array([id]*epoch)\n",
    "    one_action_labels = np.expand_dims(one_action_labels, axis=1)\n",
    "    return eeg_data, one_action_labels\n",
    "\n",
    "eeg_data, one_action_labels = read_one_subject_file(os.path.join(CLASSES[0], all_files[CLASSES[0]][0]), CLASSES_ID[CLASSES[0]])\n",
    "print(np.shape(eeg_data), np.shape(one_action_labels))\n",
    "print(np.min(eeg_data), np.max(eeg_data)) # In Old Implimentation ((3377, 64, 497), -0.698, 1)\n",
    "eeg_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(type):\n",
    "    if type not in ['train', 'valid', 'test']:  \n",
    "        raise ValueError('type should be either train, valid or test')\n",
    "    \n",
    "    dataset = list()\n",
    "    labels = list()\n",
    "    c=0\n",
    "    for dir in CLASSES:\n",
    "        for file_name in all_files[dir]:\n",
    "            c+=1\n",
    "            num = int(file_name.split('-')[0])\n",
    "            file_path = os.path.join(dir, file_name)\n",
    "            id = CLASSES_ID[dir]\n",
    "            eeg_data, one_action_labels = read_one_subject_file(file_path, id)\n",
    "            if(type == 'train' and  num < 88):\n",
    "                dataset.append(eeg_data)\n",
    "                labels.append(one_action_labels)\n",
    "\n",
    "            elif(type == 'valid' and  num >= 88 and num<98):\n",
    "                dataset.append(eeg_data)\n",
    "                labels.append(one_action_labels)\n",
    "\n",
    "            elif(type == 'test' and num >= 98):\n",
    "                dataset.append(eeg_data)\n",
    "                labels.append(one_action_labels)\n",
    "\n",
    "\n",
    "    final_data = np.vstack(dataset)\n",
    "    final_data = np.vstack(dataset)\n",
    "    final_labels = np.squeeze(np.vstack(labels))\n",
    "\n",
    "    print(c)\n",
    "    return final_data, final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2142bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate sample data\n",
    "array_3d = np.random.rand(3, 4, 5)  # Example 3D array of shape (2, 3, 4)\n",
    "array_1d = np.arange(3)  # Example 1D array of shape (8,)\n",
    "\n",
    "print(array_3d)\n",
    "print(array_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a62059",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = get_data('train')\n",
    "x_valid, y_valid = get_data('valid')\n",
    "x_test, y_test = get_data('test')\n",
    "\n",
    "# join x_train, x_valid, x_test\n",
    "x = np.concatenate((x_train, x_valid), axis=0)\n",
    "x = np.concatenate((x, x_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e621e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming train_data and train_labels are your input data and corresponding labels\n",
    "# train_data_shape = X.shape\n",
    "\n",
    "# # Generate indices for shuffling\n",
    "# np.random.seed(seed_value)\n",
    "\n",
    "# indices = np.arange(train_data_shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "\n",
    "# # Shuffle train_data and train_labels using the same indices\n",
    "# X = X[indices]\n",
    "# y = y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dee576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_info(X, y):\n",
    "    print(f'Data Shape: {X.shape}, Labels Shape: {y.shape}')\n",
    "    print(f'Min and Max of Data ({np.min(X)}, {np.max(X)})')\n",
    "    print(f'Min and Max of labels ({np.min(y)}, {np.max(y)})')\n",
    "\n",
    "\n",
    "dataset_info(x_train, y_train)\n",
    "dataset_info(x_valid, y_valid)\n",
    "dataset_info(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c18b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(y_train)):\n",
    "#     print(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1be084",
   "metadata": {
    "id": "gLp5aVzwA0tO"
   },
   "outputs": [],
   "source": [
    "class EEGDataset(data.Dataset):\n",
    "    def __init__(self, x, x_train, x_valid, x_test, y_train=None, y_valid=None, y_test=None, inference=False):\n",
    "        super().__init__()\n",
    "\n",
    "        N_SAMPLE = x.shape[0]\n",
    "        \n",
    "        if not inference:\n",
    "            self.train_ds = {\n",
    "                'x': x_train,\n",
    "                'y': y_train,\n",
    "            }\n",
    "            # print(self.train_ds['x'].shape)\n",
    "            \n",
    "            self.val_ds = {\n",
    "                'x': x_valid,\n",
    "                'y': y_valid,\n",
    "            }\n",
    "            # print(self.val_ds['x'].shape)\n",
    "            \n",
    "            self.test_ds = {\n",
    "                'x': x_test,\n",
    "                'y': y_test,\n",
    "            }\n",
    "            # print(self.test_ds['x'].shape)\n",
    "        else:\n",
    "            self.__split = \"inference\"\n",
    "            self.inference_ds = {\n",
    "                'x': [x],\n",
    "            }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset['x'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x = self.dataset['x'][idx]\n",
    "        if self.__split != \"inference\":\n",
    "            y = self.dataset['y'][idx]\n",
    "            x = torch.tensor(x).float()\n",
    "            # y = torch.tensor(y).unsqueeze(-1).float()\n",
    "            y = torch.tensor(y).float()\n",
    "            return x, y\n",
    "        else:\n",
    "            x = torch.tensor(x).float()\n",
    "            return x\n",
    "\n",
    "    def split(self, __split):\n",
    "        self.__split = __split\n",
    "        return self\n",
    "\n",
    "    @classmethod\n",
    "    def inference_dataset(cls, x):\n",
    "        return cls(x, inference=True)\n",
    "\n",
    "    @property\n",
    "    def dataset(self):\n",
    "        assert self.__split is not None, \"Please specify the split of dataset!\"\n",
    "\n",
    "        if self.__split == \"train\":\n",
    "            return self.train_ds\n",
    "        elif self.__split == \"val\":\n",
    "            return self.val_ds\n",
    "        elif self.__split == \"test\":\n",
    "            return self.test_ds\n",
    "        elif self.__split == \"inference\":\n",
    "            return self.inference_ds\n",
    "        else:\n",
    "            raise TypeError(\"Unknown type of split!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b51a2",
   "metadata": {
    "id": "Q0PANUZsComS"
   },
   "outputs": [],
   "source": [
    "eeg_dataset = EEGDataset(x=x, x_train=x_train, x_valid=x_valid, x_test=x_test, y_train=y_train, y_valid=y_valid, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7f8fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "XgHrQ_Nvie1A",
    "outputId": "85e41dc7-42c8-4a49-9551-c0b8552fc6f9"
   },
   "outputs": [],
   "source": [
    "# plt.plot(X[18:21, 0, :].T)\n",
    "# plt.title(\"Exemplar single-trial epoched data, for electrode 0\")\n",
    "# plt.ylabel(\"V\")\n",
    "# plt.xlabel(\"Epoched Sample\")\n",
    "# plt.show()\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bbca17",
   "metadata": {
    "id": "jcjNYuJYn8Ox"
   },
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b9129",
   "metadata": {
    "id": "NImfOgC4fy3v"
   },
   "source": [
    "### **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04edb4a5",
   "metadata": {
    "id": "DrmSbEY9n8Tg"
   },
   "outputs": [],
   "source": [
    "class AvgMeter(object):\n",
    "    def __init__(self, num=40):\n",
    "        self.num = num\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.losses = []\n",
    "\n",
    "    def update(self, val):\n",
    "        self.losses.append(val)\n",
    "\n",
    "    def show(self):\n",
    "        out = torch.mean(\n",
    "            torch.stack(\n",
    "                self.losses[np.maximum(len(self.losses)-self.num, 0):]\n",
    "            )\n",
    "        )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c9dca",
   "metadata": {
    "id": "G6maL6CifzWk"
   },
   "source": [
    "### **Wrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f9909b",
   "metadata": {
    "id": "gdmdnCNcfzdN"
   },
   "outputs": [],
   "source": [
    "class ModelWrapper(L.LightningModule):\n",
    "    def __init__(self, arch, dataset, batch_size, lr, max_epoch):\n",
    "        super().__init__()\n",
    "\n",
    "        self.arch = arch\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.max_epoch = max_epoch\n",
    "\n",
    "        # self.train_accuracy = Accuracy(task=\"binary\") # change\n",
    "        # self.val_accuracy = Accuracy(task=\"binary\")\n",
    "        # self.test_accuracy = Accuracy(task=\"binary\")\n",
    "        \n",
    "        \n",
    "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=4)\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=4)\n",
    "        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=4)\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "\n",
    "        self.train_loss_recorder = AvgMeter()\n",
    "        self.val_loss_recorder = AvgMeter()\n",
    "\n",
    "        self.train_acc_recorder = AvgMeter()\n",
    "        self.val_acc_recorder = AvgMeter()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.arch(x)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y=y.to(torch.int64)\n",
    "        y_hat = self(x)\n",
    "        # loss = F.binary_cross_entropy_with_logits(y_hat, y) # change\n",
    "        loss = F.cross_entropy(y_hat, y) # change\n",
    "        # loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        self.train_accuracy.update(y_hat, y)\n",
    "        acc = self.train_accuracy.compute().data.cpu()\n",
    "        # print(loss, acc) # temp\n",
    "        opt = self.optimizers()\n",
    "        opt.zero_grad()\n",
    "        self.manual_backward(loss)\n",
    "        opt.step()\n",
    "\n",
    "        self.train_loss_recorder.update(loss.data)\n",
    "        self.train_acc_recorder.update(acc)\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        sch = self.lr_schedulers()\n",
    "        sch.step()\n",
    "\n",
    "        self.train_loss.append(self.train_loss_recorder.show().data.cpu().numpy())\n",
    "        self.train_loss_recorder = AvgMeter()\n",
    "\n",
    "        self.train_acc.append(self.train_acc_recorder.show().data.cpu().numpy())\n",
    "        self.train_acc_recorder = AvgMeter()\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y=y.to(torch.int64)\n",
    "        y_hat = self(x)\n",
    "        # loss = F.binary_cross_entropy_with_logits(y_hat, y) # change\n",
    "        loss = F.cross_entropy(y_hat, y) # change\n",
    "        # loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "\n",
    "        self.val_accuracy.update(y_hat, y)\n",
    "        acc = self.val_accuracy.compute().data.cpu()\n",
    "\n",
    "        self.val_loss_recorder.update(loss.data)\n",
    "        self.val_acc_recorder.update(acc)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.val_loss.append(self.val_loss_recorder.show().data.cpu().numpy())\n",
    "        self.val_loss_recorder = AvgMeter()\n",
    "\n",
    "        self.val_acc.append(self.val_acc_recorder.show().data.cpu().numpy())\n",
    "        self.val_acc_recorder = AvgMeter()\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y=y.to(torch.int64)\n",
    "        y_hat = self(x)\n",
    "        # loss = F.binary_cross_entropy_with_logits(y_hat, y) # change\n",
    "        loss = F.cross_entropy(y_hat, y) # change\n",
    "        # loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        self.test_accuracy.update(y_hat, y)\n",
    "\n",
    "        self.log(\n",
    "            \"test_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_acc\",\n",
    "            self.test_accuracy.compute(),\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def on_train_end(self):\n",
    "        # Loss \n",
    "        loss_img_file = \"content/loss_plot.png\"\n",
    "        plt.plot(self.train_loss, color = 'r', label='train')\n",
    "        plt.plot(self.val_loss, color = 'b', label='validation')\n",
    "        plt.title(\"Loss Curves\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.savefig(loss_img_file)\n",
    "        plt.clf()\n",
    "        img = cv2.imread(loss_img_file)\n",
    "        cv2_imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "        # Accuracy\n",
    "        acc_img_file = \"content/acc_plot.png\"\n",
    "        plt.plot(self.train_acc, color = 'r', label='train')\n",
    "        plt.plot(self.val_acc, color = 'b', label='validation')\n",
    "        plt.title(\"Accuracy Curves\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.savefig(acc_img_file)\n",
    "        plt.clf()\n",
    "        img = cv2.imread(acc_img_file)\n",
    "        cv2_imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(\n",
    "            dataset=self.dataset.split(\"train\"),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(\n",
    "            dataset=self.dataset.split(\"val\"),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return data.DataLoader(\n",
    "            dataset=self.dataset.split(\"test\"),\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "        )\n",
    "        lr_scheduler = {\n",
    "            \"scheduler\": optim.lr_scheduler.MultiStepLR(\n",
    "                optimizer,\n",
    "                milestones=[\n",
    "                    int(self.max_epoch * 0.25),\n",
    "                    int(self.max_epoch * 0.5),\n",
    "                    int(self.max_epoch * 0.75),\n",
    "                ],\n",
    "                gamma=0.1\n",
    "            ),\n",
    "            \"name\": \"lr_scheduler\",\n",
    "        }\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd54684d",
   "metadata": {
    "id": "zmKcJRmqf3RT"
   },
   "source": [
    "### **EEG Classification Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034032eb",
   "metadata": {
    "id": "9vZNgUPzPC2g"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding.\n",
    "    https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html\n",
    "    \"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.p = torch.zeros((1, max_len, num_hiddens))\n",
    "        x = torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1) / torch.pow(10000, torch.arange(\n",
    "            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        self.p[:, :, 0::2] = torch.sin(x)\n",
    "        self.p[:, :, 1::2] = torch.cos(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.p[:, :x.shape[1], :].to(x.device)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1bd2d3",
   "metadata": {
    "id": "1kuYblfPluNR"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, dim_feedforward),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, embed_dim),\n",
    "        )\n",
    "\n",
    "        self.layernorm0 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        y, att = self.attention(x, x, x)\n",
    "        y = F.dropout(y, self.dropout, training=self.training)\n",
    "        x = self.layernorm0(x + y)\n",
    "        y = self.mlp(x)\n",
    "        y = F.dropout(y, self.dropout, training=self.training)\n",
    "        x = self.layernorm1(x + y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d5d76",
   "metadata": {
    "id": "-Az5aZQcf3Vf"
   },
   "outputs": [],
   "source": [
    "class EEGClassificationModel(nn.Module):\n",
    "    def __init__(self, eeg_channel, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                eeg_channel, eeg_channel, 11, 1, padding=5, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm1d(eeg_channel),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout1d(dropout),\n",
    "            nn.Conv1d(\n",
    "                eeg_channel, eeg_channel * 2, 11, 1, padding=5, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm1d(eeg_channel * 2),\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.Sequential(\n",
    "            PositionalEncoding(eeg_channel * 2, dropout),\n",
    "            TransformerBlock(eeg_channel * 2, 4, eeg_channel // 8, dropout),\n",
    "            TransformerBlock(eeg_channel * 2, 4, eeg_channel // 8, dropout),\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(eeg_channel * 2, eeg_channel // 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "            # nn.Linear(eeg_channel // 2, 1), # change\n",
    "            nn.Linear(eeg_channel // 2, 4), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = x.mean(dim=-1)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e1f41",
   "metadata": {
    "id": "7pqVobuLg8AJ"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"EEGClassificationModel\"\n",
    "model = EEGClassificationModel(eeg_channel=EEG_CHANNEL, dropout=0.125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83407393",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torchviz import make_dot\n",
    "\n",
    "# # Generate random data\n",
    "# x = torch.randn(12, 64, 497)\n",
    "# print(f'Input Shape {x.shape}')\n",
    "\n",
    "# # Print shape\n",
    "\n",
    "\n",
    "# # generate predictions for the sample data\n",
    "# y = model(x)\n",
    "# print(f'Output Shape {y.shape}')\n",
    "\n",
    "# # generate a model architecture visualization\n",
    "# make_dot(y,\n",
    "#          params=dict(model.named_parameters()),\n",
    "#          show_attrs=True,\n",
    "#          show_saved=True).render(\"MyPyTorchModel_torchviz\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e95c842",
   "metadata": {
    "id": "XrT9CzvNn8Zj"
   },
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ada7b0a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhXydlHxn8d2",
    "outputId": "c81a0aac-2c2a-4459-8d0e-a3b887cedf70"
   },
   "outputs": [],
   "source": [
    "MAX_EPOCH = 100\n",
    "BATCH_SIZE = 32\n",
    "LR = 5e-4\n",
    "CHECKPOINT_DIR = os.getcwd()\n",
    "# SEED = int(np.random.randint(2147483647))\n",
    "SEED = 141352557\n",
    "\n",
    "print(f\"Random seed: {SEED}\")\n",
    "\n",
    "model_w = ModelWrapper(model, eeg_dataset, BATCH_SIZE, LR, MAX_EPOCH)\n",
    "\n",
    "\n",
    "!rm -rf logs/\n",
    "\n",
    "\n",
    "\n",
    "tensorboardlogger = TensorBoardLogger(save_dir=\"logs/\")\n",
    "csvlogger = CSVLogger(save_dir=\"logs/\")\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "checkpoint = ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath=CHECKPOINT_DIR,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "# early_stopping = EarlyStopping(\n",
    "#     monitor=\"val_acc\", min_delta=0.00, patience=3, verbose=False, mode=\"max\"\n",
    "# )\n",
    "\n",
    "\n",
    "seed_everything(SEED, workers=True)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    logger=[tensorboardlogger, csvlogger],\n",
    "    # callbacks=[lr_monitor, checkpoint, early_stopping],\n",
    "    callbacks=[lr_monitor, checkpoint], # exp\n",
    "    log_every_n_steps=5,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f321b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before', model_w.device)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model_w.to(device)\n",
    "print('After', model_w.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a100367f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,3\"  # specify which GPU(s) to be used\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3,4\"  # specify which GPU(s) to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_w = ModelWrapper.load_from_checkpoint(checkpoint_path=\"epoch=19-step=19020.ckpt\", arch=EEGClassificationModel(eeg_channel=EEG_CHANNEL, dropout=0.125), dataset=eeg_dataset, batch_size=BATCH_SIZE, lr=LR, max_epoch=MAX_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35735cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\") ## specify the GPU id's, GPU id's start from 0.\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7196c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_w= nn.DataParallel(model_w, device_ids = [1, 3])\n",
    "# model_w= nn.parallel.DistributedDataParallel(model_w, device_ids = [1, 3])\n",
    "# model_w.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c1ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46443cc",
   "metadata": {
    "id": "ljCtuHhvgHVn"
   },
   "source": [
    "## **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e0e6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249,
     "referenced_widgets": [
      "1bc5ff501ec5408b9acb21d96a73cdd3",
      "0037244073884f838214fbd9a8ee9302",
      "338c2e2598f94e7b9666e65b5511ccb2",
      "66aeeec34f1a4612a0ff9e6723dbba75",
      "51599653ba3e4d26985ff39ef0cbc654",
      "b237ad9bf9574c74a946af74fdb46e7c",
      "23be9d3ca285471eb783de14b9b60858",
      "3d44f149623245b0808df6bbf7f8fb72",
      "94edc4d6045b4e4db459c14729e89e4d",
      "b76cfb981e9342558774ec9dd10eed59",
      "eb57eb68021940ca98c7b8ab2b48f4cf"
     ]
    },
    "id": "kwUGPu6tgHbU",
    "outputId": "bee03dae-a6eb-46dd-b56a-f92b8537bd2a"
   },
   "outputs": [],
   "source": [
    "# trainer.test(ckpt_path=\"EEGClassificationModel_best.ckpt\")\n",
    "# trainer.test(model=model_w ,ckpt_path=\"epoch=6-step=1666.ckpt\")\n",
    "\n",
    "\n",
    "# os.rename(\n",
    "#     checkpoint.best_model_path,\n",
    "#     os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}_best.ckpt\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b68156",
   "metadata": {
    "id": "nI1f_6OmgMQB"
   },
   "source": [
    "## **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(sample: np.ndarray, model_wrappr: L.LightningModule) -> list:  #(samples_count, EEG_CHANNEL, Data_Points)\n",
    "    if(sample.shape == 2):\n",
    "        sample = np.expand_dims(sample, 0)\n",
    "    sample  = torch.from_numpy(sample)\n",
    "    print(f'Input Size: {np.shape(sample)}') # (samples_count, EEG_CHANNEL, Data_Points)\n",
    "    trainer = Trainer()\n",
    "    pred = trainer.predict(model=model_wrappr, \n",
    "                        dataloaders=data.DataLoader(\n",
    "                        dataset = sample,\n",
    "                        batch_size=1,\n",
    "                        shuffle=False,\n",
    "                    )\n",
    "            )\n",
    "    pred = torch.tensor(np.array(pred))\n",
    "    print(f'Output Size: {np.shape(pred)}') # (samples_count, 1, classes)\n",
    "    pred = torch.softmax(pred, dim=2)\n",
    "    # print(pred)\n",
    "    predicted_class = torch.squeeze(pred.argmax(dim=2))\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_dataset_test = eeg_dataset.split(\"test\")\n",
    "print(np.shape(eeg_dataset_test.dataset['x']))\n",
    "# eeg_dataset_test.dataset['x']\n",
    "# eeg_dataset_test.dataset['y']\n",
    "\n",
    "y_true = eeg_dataset_test.dataset['y']\n",
    "y_pred = prediction(eeg_dataset_test.dataset['x'], model_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90c6bb",
   "metadata": {},
   "source": [
    "## **Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perital and ocpital channel\n",
    "# Note: The following confusion matrix code is a remix of Scikit-Learn's \n",
    "# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\n",
    "# and Made with ML's introductory notebook - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb\n",
    "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=6): \n",
    "  \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n",
    "\n",
    "  If classes is passed, confusion matrix will be labelled, if not, integer class values\n",
    "  will be used.\n",
    "\n",
    "  Args:\n",
    "    y_true: Array of truth labels (must be same shape as y_pred).\n",
    "    y_pred: Array of predicted labels (must be same shape as y_true).\n",
    "    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n",
    "    figsize: Size of output figure (default=(10, 10)).\n",
    "    text_size: Size of output figure text (default=15).\n",
    "  \n",
    "  Returns:\n",
    "    A labelled confusion matrix plot comparing y_true and y_pred.\n",
    "\n",
    "  Example usage:\n",
    "    make_confusion_matrix(y_true=test_Y, # ground truth test labels\n",
    "                          y_pred=y_preds, # predicted labels\n",
    "                          classes=class_names, # array of class label names\n",
    "                          figsize=(15, 15),\n",
    "                          text_size=10)\n",
    "  \"\"\"  \n",
    "  # Create the confustion matrix\n",
    "  cm = confusion_matrix(y_true, y_pred)\n",
    "  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n",
    "  n_classes = cm.shape[0] # find the number of classes we're dealing with\n",
    "\n",
    "  # Plot the figure and make it pretty\n",
    "  fig, ax = plt.subplots(figsize=figsize)\n",
    "  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n",
    "  fig.colorbar(cax)\n",
    "\n",
    "  # Are there a list of classes?\n",
    "  if classes:\n",
    "    labels = classes\n",
    "  else:\n",
    "    labels = np.arange(cm.shape[0])\n",
    "  \n",
    "  # Label the axes\n",
    "  ax.set(title=\"Confusion Matrix\",\n",
    "         xlabel=\"Predicted label\",\n",
    "         ylabel=\"True label\",\n",
    "         xticks=np.arange(n_classes), # create enough axis slots for each class\n",
    "         yticks=np.arange(n_classes), \n",
    "         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n",
    "         yticklabels=labels)\n",
    "  \n",
    "  # Make x-axis labels appear on bottom\n",
    "  ax.xaxis.set_label_position(\"bottom\")\n",
    "  ax.xaxis.tick_bottom()\n",
    "\n",
    "  # Set the threshold for different colors\n",
    "  threshold = (cm.max() + cm.min()) / 2.\n",
    "\n",
    "  # Plot the text on each cell\n",
    "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > threshold else \"black\",\n",
    "             size=text_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef11e073",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(y_true, y_pred, classes=CLASSES, figsize=(20, 20), text_size=20)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
